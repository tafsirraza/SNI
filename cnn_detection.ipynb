{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import pywt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from obspy import read\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from scipy.ndimage import median_filter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from obspy.signal.filter import bandpass\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_cat = 'data/lunar/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "lunar_df = pd.read_csv(lunar_cat)\n",
    "\n",
    "lunar_event = {}\n",
    "for index, row in lunar_df.iterrows():\n",
    "\n",
    "    filename = row['filename']\n",
    "    cleaned_filename = re.match(r\"^.*\\d{4}-\\d{2}-\\d{2}\", filename).group()\n",
    "    \n",
    "    detection_time = row['time_rel(sec)']\n",
    "    \n",
    "    if cleaned_filename in lunar_event:\n",
    "        lunar_event[cleaned_filename].append(detection_time)\n",
    "    else:\n",
    "        lunar_event[cleaned_filename] = [detection_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_transform(data, wavelet, scales):\n",
    "    coefficients, frequencies = pywt.cwt(data, scales, wavelet)\n",
    "    return np.abs(coefficients), frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace(tr_times, tr_data, arrival, coefficients, scales):\n",
    "    fig, (ax, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "    ax.plot(tr_times,tr_data)\n",
    "\n",
    "    ax.axvline(x = arrival, color='red',label='Rel. Arrival')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    ax.set_xlim([min(tr_times),max(tr_times)])\n",
    "    ax.set_ylabel('Velocity (m/s)')\n",
    "    ax.set_title('Seismic Trace', fontweight='bold')\n",
    "\n",
    "    ax2.imshow(coefficients, extent=[tr_times.min(), tr_times.max(), scales.min(), scales.max()],\n",
    "               aspect='auto', interpolation='bilinear', cmap='jet')\n",
    "    ax2.set_ylabel('Scales')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_title('Wavelet Coefficients', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mseed(file_path, arrival, minfreq=0.5, maxfreq=1.5):\n",
    "    wavelet='cmor1.0-0.5' #adjust the center frequency (0.5 to 10 Hz) and bandwidth (around 1.5)\n",
    "    scales = np.arange(1, 20)\n",
    "\n",
    "    st = read(file_path)    \n",
    "    tr = st[0]\n",
    "    if np.count_nonzero(np.isnan(tr.data)) > 0:\n",
    "        print(f\"Warning: Missing values found in {tr.id}. Interpolation may be needed.\")\n",
    "        tr.interpolate(method='linear', tolerance=0.1, sampling_rate=tr.stats.sampling_rate)\n",
    "    \n",
    "    data = tr.data.reshape(-1, 1)\n",
    "    tr_times = tr.times()\n",
    "\n",
    "    tr.data = data.flatten()\n",
    "    tr.filter(\"bandpass\", freqmin=minfreq, freqmax=maxfreq, corners=4, zerophase=True)\n",
    "    \n",
    "    coefficients, frequencies = wavelet_transform(tr.data, wavelet, scales)\n",
    "    \n",
    "    # plot_trace(tr_times, tr.data, arrival, coefficients, scales) #Use this to check the plot for Seismic trace and Waveleet Coefficients\n",
    "    return tr.data, coefficients, os.path.basename(file_path), tr_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, event_time):\n",
    "    try:\n",
    "        filtered_data, wavelet_coefficients, filename, timeline = preprocess_mseed(file_path, event_time)\n",
    "        return filename, filtered_data, wavelet_coefficients, timeline\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(filename):\n",
    "    match = re.match(r\"^.*\\d{4}-\\d{2}-\\d{2}\", filename)\n",
    "    if match:\n",
    "        return match.group()  \n",
    "    else:\n",
    "        return filename  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wavelet_coefficients_with_time(coefficients, timeline, threshold=0.2):\n",
    "    # Compute the threshold based on a percentage of the maximum value in the coefficients\n",
    "    max_value = np.max(np.abs(coefficients))\n",
    "    significant_mask = np.abs(coefficients) >= (threshold * max_value)\n",
    "    \n",
    "    # Retain only coefficients that have significant values\n",
    "    filtered_coefficients = coefficients * significant_mask\n",
    "    \n",
    "    # Keep only the relevant time indices where significant coefficients exist\n",
    "    relevant_time_steps = [timeline[i] for i in range(coefficients.shape[1]) if np.any(significant_mask[:, i])]\n",
    "    filtered_coefficients = filtered_coefficients[:, significant_mask.any(axis=0)]\n",
    "    \n",
    "    return filtered_coefficients, relevant_time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseed_training_directory = 'data/lunar/training/data/S12_GradeA'\n",
    "mseed_testing_directory = 'data/lunar/test/data/'\n",
    "\n",
    "'''\n",
    "Looping through each mseed file to apply data preprocessing.\n",
    "'''\n",
    "\n",
    "\n",
    "cnn_train_input = []\n",
    "train_labels = []\n",
    "\n",
    "cnn_test_input = []\n",
    "for filename in os.listdir(mseed_training_directory):\n",
    "    if filename.endswith(\".mseed\"):\n",
    "        file_path = os.path.join(mseed_training_directory, filename)\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        cleaned_filename = clean_filename(filename)\n",
    "        filename, filtered_data, wavelet_coefficients, timeline = process_file(file_path, lunar_event[cleaned_filename])\n",
    "        \n",
    "        # filtered_coefficients, relevant_time_steps = filter_wavelet_coefficients_with_time(wavelet_coefficients, timeline)\n",
    "        \n",
    "\n",
    "        # cnn_train_input.append(filtered_coefficients)\n",
    "        cnn_train_input.append(wavelet_coefficients)\n",
    "        # Assuming the label is the seismic event arrival time, which should be within the relevant time steps\n",
    "        arrival_times = lunar_event[cleaned_filename]  # Adjust according to your label\n",
    "        train_labels.append(arrival_times)\n",
    "\n",
    "for directory in os.listdir(mseed_testing_directory):\n",
    "    for filename in os.listdir(os.path.join(mseed_testing_directory, directory)):\n",
    "        if filename.endswith(\".mseed\"):\n",
    "            file_path = os.path.join(mseed_testing_directory, directory, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            cleaned_filename = clean_filename(filename)\n",
    "            filename, filtered_data, wavelet_coefficients, timeline = process_file(file_path, 0)\n",
    "            \n",
    "            # filtered_coefficients, relevant_time_steps = filter_wavelet_coefficients_with_time(wavelet_coefficients, timeline)\n",
    "            \n",
    "\n",
    "            # cnn_test_input.append(filtered_coefficients)\n",
    "            cnn_test_input.append(wavelet_coefficients)\n",
    "    break\n",
    "\n",
    "# After looping through all files, determine the maximum time steps\n",
    "max_time_steps = max(coeff.shape[1] for coeff in cnn_train_input)\n",
    "max_time_steps_test = max(coeff.shape[1] for coeff in cnn_test_input)\n",
    "\n",
    "# Pad the coefficients\n",
    "padded_coefficients = []\n",
    "for coeff in cnn_train_input:\n",
    "    padded = np.pad(coeff, ((0, 0), (0, max_time_steps - coeff.shape[1])), mode='constant', constant_values=0)\n",
    "    padded_coefficients.append(padded)\n",
    "\n",
    "padded_test_coefficients = []\n",
    "for coeff in cnn_test_input:\n",
    "    padded = np.pad(coeff, ((0, 0), (0, max_time_steps_test - coeff.shape[1])), mode='constant', constant_values=0)\n",
    "    padded_test_coefficients.append(padded)\n",
    "\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "cnn_train_input = np.array(padded_coefficients)\n",
    "cnn_test_input = np.array(padded_test_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_events = max(len(arrival) for arrival in train_labels)  # Determine the maximum number of events\n",
    "train_labels_padded = []\n",
    "\n",
    "for arrival in train_labels:\n",
    "    padded = np.pad(arrival, (0, max_events - len(arrival)), 'constant')\n",
    "    train_labels_padded.append(padded)\n",
    "\n",
    "train_labels = np.array(train_labels_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cnn_train_input shape:\", cnn_train_input.shape)  # Should be (num_samples, 19, max_time_steps)\n",
    "print(\"train_labels shape:\", train_labels.shape)  # Should be (num_samples, max_events)\n",
    "\n",
    "# Reshape the input for CNN (num_samples, height, width, channels)\n",
    "cnn_train_input = cnn_train_input.reshape(cnn_train_input.shape[0], cnn_train_input.shape[1], cnn_train_input.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# First Convolutional Layer\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(19, 572427, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(1, 10)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())  # Added Batch Normalization\n",
    "\n",
    "# Second Convolutional Layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 10)))  # Further downsampling\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())  # Added Batch Normalization\n",
    "\n",
    "# Global Average Pooling Layer\n",
    "model.add(GlobalAveragePooling2D())  # Use global average pooling\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout to reduce overfitting\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(1, activation='linear')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "history = model.fit(cnn_train_input, train_labels, epochs=10, batch_size=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(cnn_test_input)\n",
    "\n",
    "# Print predicted arrival times\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
